{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stoffprof/X615/blob/main/Wordle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX1kk7T7VxOc"
      },
      "source": [
        "# Solving a word puzzle\n",
        "\n",
        "In 2018 the *New York Times* introduced a new word game called Spelling Bee. The game board has six letters plus a center letter, and the objective is to construct as many words as possible containing at least the center letter.\n",
        "\n",
        "As an example, consider this game board:\n",
        "<p><img src=\"https://raw.githubusercontent.com/stoffprof/X615/refs/heads/main/images/spelling_bee.png\" alt=\"Alt text\" width=\"200\">\n",
        "\n",
        "\n",
        "Some possible words include *coat*, *bobcat*, and *jackboot*. As is clear from these examples, letters may be reused. Words must be at least four letters. Four-letter words get one point, and longer words earn one point per letter, with a bonus 7 points for \"pangrams\" that use all letters.\n",
        "\n",
        "Let's apply what we've learned about data structures to come up with an algorithm that can solve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a word corpus\n",
        "\n",
        "To begin, we need a list of valid words against which we can check a candidate solution. For example, *coat* is a word, but *attcao* isn't; the only way to know this is by checking a list of words.\n",
        "\n",
        "If we had the contents of, say, the Merriem-Webster dictionary on our computer to look up possible words, we could use that. It is possible to purchase access to dictionaries like that, but there is a free alternative. A number of lists of words, or *corpora*, have been created. Many of these were developed to study how words are used in different contexts. One such corpus is the [Google ngram data](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html), which provides counts of how frequently different unigrams, bigrams, or higher-order *n*-grams appear on the internet. (In computational linguistics, a *unigram* is a single sequence from a body of text, like \"word\", \"3,705\" or \"ID_054\". An example of a bigram is \"White House\".)\n",
        "\n",
        "Using unigrams from these data solves one problem--they're free and easy to get--but creates another, which is that not all unigrams are legitimate English words; since they're just pieces of text that appear on web sites, they also include misspellings of words or other short strings."
      ],
      "metadata": {
        "id": "bE_3MKpCY9uZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L26y9ZY1VxOc"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = ('https://github.com/norvig/pytudes/raw/refs/heads/main/data/text/count_1w.txt')\n",
        "inf = requests.get(url).text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n'.join(inf.splitlines()[:5]))"
      ],
      "metadata": {
        "id": "MtXZB8UtaQ1D",
        "outputId": "51b7423a-2af3-40cf-a4c5-83f93814d6c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\t23135851162\n",
            "of\t13151942776\n",
            "and\t12997637966\n",
            "to\t12136980858\n",
            "a\t9081174698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38CszjBbVxOc"
      },
      "source": [
        "The file has one record on each line, with the unigram follow by a tab character and the number of times the unigram appears in the corpus. The first record indicates that the word *the* appeared more than 23 billion times on the web pages.\n",
        "\n",
        "To work with these data, it will be helpful to create a list of the records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nqjarv8vVxOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e1f1cf-c2d2-443c-de90-7f147d3f2418"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 23135851162),\n",
              " ('of', 13151942776),\n",
              " ('and', 12997637966),\n",
              " ('to', 12136980858),\n",
              " ('a', 9081174698),\n",
              " ('in', 8469404971),\n",
              " ('for', 5933321709),\n",
              " ('is', 4705743816),\n",
              " ('on', 3750423199),\n",
              " ('that', 3400031103)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from collections import Counter\n",
        "unigrams = Counter()\n",
        "\n",
        "for rec in inf.rstrip().split('\\n'):\n",
        "    w,c = rec.split()\n",
        "    unigrams[w] = int(c)\n",
        "\n",
        "unigrams.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5fwabkPVxOc"
      },
      "source": [
        "The original [n-gram corpus](https://catalog.ldc.upenn.edu/LDC2006T13) contains over 13 million unigrams, but these data are a subset that was created to be more manageable, mostly by excluding unigrams that are quite unlikely to be actual words. It includes only the first one-third of a million ngrams, sorted in descending order of frequency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CIOe3DY_VxOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b185eaea-4812-45df-ddd7-cd7f533050f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "333333"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(unigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_onjohVVxOc"
      },
      "source": [
        "The `most_common` method prints all elements in decreasing order of their associated count value, so looking at last elements will show us which are the least common."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "X2cx8okJVxOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63be47ef-d7c1-4c2c-d33b-b5fd1dc750df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('googllr', 12711),\n",
              " ('googlal', 12711),\n",
              " ('googgoo', 12711),\n",
              " ('googgol', 12711),\n",
              " ('goofel', 12711),\n",
              " ('gooek', 12711),\n",
              " ('gooddg', 12711),\n",
              " ('gooblle', 12711),\n",
              " ('gollgo', 12711),\n",
              " ('golgw', 12711)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "unigrams.most_common()[-10:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGXJmjxrVxOc"
      },
      "source": [
        "Clearly the corpus contains a lot of unigrams that aren't real words. Since it was created automatically from web page content and isn't systematically curated, there are a lot of things on the list that aren't actually words.\n",
        "\n",
        "We can try to remove entries that are either not real words, or are extremely obscure, by keeping only those that appear more than, say, 100,000 times. This cutoff is obviously arbitrary. Some non-words likely appear more than this, while some real words may not meet this cutoff. For example, here's a long word that is really a word, but isn't used that frequently online."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fuj2tRfnVxOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef990470-e903-448b-e319-738a14cda893"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16489"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "unigrams['antidisestablishmentarianism']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mezF9s_iVxOd"
      },
      "outputs": [],
      "source": [
        "wordlist = [word for word,count in unigrams.items() if count>100000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ku6ZFVcVxOd"
      },
      "source": [
        "Because we're unpacking the (*word*, *count*) pairs we need to be careful to remove the last entry in the list to avoid getting a \"not enough values to unpack\" error, as it only has one entry. We're left with a list of just under 100,000 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vYG-MVkWVxOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d1740c-c21a-48bf-a947-0f57bb36f266"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99487"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(wordlist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUYfpYxUVxOd"
      },
      "source": [
        "There is a little more pruning of our list we can do. First, some unigrams have no vowels, but any real word needs them. Here's a unigram that appears quite frequently but isn't a word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J-2XuNkzVxOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14d8b70c-fa24-440d-add1-afdb67ed2b12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2710841"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "unigrams['bbb']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crBKG6y7VxOd"
      },
      "source": [
        "To filter these out, we'll make a small function that checks if a word contains any vowels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3zcDOf8LVxOd"
      },
      "outputs": [],
      "source": [
        "def has_vowel(word):\n",
        "    return any([v in word for v in list('aeiou')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9PMu-z1PVxOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1841a0e-0653-4c88-8760-4a8e10d0d9c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95038"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "wordlist = list(filter(has_vowel, wordlist))\n",
        "len(wordlist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ih6DVIlVxOd"
      },
      "source": [
        "That removed about 4,000 unigrams. Another restriction we can impose is that words be at least four letters long, since this is one of the rules of the puzzle. This removes about 5,000 more words from our list, leaving us with just over 90,000 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OO7BBR-TVxOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5789048-d4c3-42a6-f694-19a0acea605e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90223"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "wordlist = [word for word in wordlist if len(word)>3]\n",
        "len(wordlist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrE43d2HVxOd"
      },
      "source": [
        "## A word-finding algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8vOV9P1VxOd"
      },
      "source": [
        "How can we go about finding word candidates that appear on the list? A brute force approach would be to generate possible words and check if they are in the dictionary. While this could work, it would be incredibly inefficient; if we only check words up to ten letters long, there are still about 330 million possible words to check:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[7**k for k in range(1,11)]"
      ],
      "metadata": {
        "id": "7eCW7bAzanHR",
        "outputId": "7d7d80c3-02b4-454d-f500-48b1dcb7b6f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 49, 343, 2401, 16807, 117649, 823543, 5764801, 40353607, 282475249]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lfrjsoUGVxOd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59832d01-80e7-4872-9ee2-c30a304f6a4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'329,554,456'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "'{:,d}'.format(sum([7**k for k in range(1,11)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIllrutVVxOd"
      },
      "source": [
        "How long would it take to check whether 330 million possible words are in the dictionary? We can estimate this by checking how long it takes to look up one word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "R6tUNdM1VxOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9181636f-5ecd-4274-fb5f-0e9ae9b2bce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.34 ms ± 238 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
          ]
        }
      ],
      "source": [
        "%timeit 'belicose' in wordlist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "err0ORrpVxOd"
      },
      "source": [
        "The `%timeit` macro repeatedly executes a piece of code and times how long each execution takes. In this case, it checked whether *belicose* is in the list 1000 times, and found that on average it took about one millisecond. That's pretty fast, but if we had to to it 330 million times, it would take almost four days:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "b2TG3-UKVxOe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7f94a0c-1b88-47ed-f494-39612a9bfb01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.814287696759259"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# convert milliseconds to days\n",
        "329554457 / 1000 / 60 / 60 / 24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfgKgGraVxOe"
      },
      "source": [
        "Clearly, we need a better approach. One idea is to preprocess our word list to construct a list of words that can be constructed from all possible letter groupings. For example, we might have:\n",
        "\n",
        "`words[('a','b','c','k')] = ['back', 'aback']`\n",
        "\n",
        "`words[('b','e','l','t')] = ['belt', 'beetle']`\n",
        "\n",
        "We could then generate all possible unique combinations of our 7 letters, and then simply look in our dictionary for words that match the necessary pattern. We can easily construct such a dictionary from our word list by using a `defaultdict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnaVbE_SVxOe"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "words = defaultdict(list)\n",
        "for word in wordlist:\n",
        "    words[tuple(sorted(set(word)))].append(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWjPypn7VxOe"
      },
      "source": [
        "To see how this works, consider each of the pieces of the code. We iterate over our list of words, assigning each one to `word` one-at-a-time. We then call `set()` on `word`, which will return a set of the unique letters in the word. (Recall that `set` takes an iterable--in this case a string--and creates a set with the unique elements.) We then sort the elements of the set, which returns a list, and finally convert this list to a tuple so it can be used as the key in the `words` dictionary. The value corresponding to this key is a list, and we append `word` to this list.\n",
        "\n",
        "Having done this, we have a dictionary of 47,500 possible letter combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIepuU-jVxOe"
      },
      "outputs": [],
      "source": [
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTYxS7HNVxOe"
      },
      "source": [
        "Let's look at a few examples to see what the dictionary looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epNQ4tu1VxOe"
      },
      "outputs": [],
      "source": [
        "words[('a', 'b', 'c', 'k')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdkQgDJxVxOe"
      },
      "outputs": [],
      "source": [
        "words[('a', 'b', 'c', 'k', 'l')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjWETci2VxOe"
      },
      "source": [
        "We obviously didn't get rid of all non-words with our filters. *balck* is likely a misspelling of *black* that appears sufficiently frequently on the internet that it has made it into our list. Similarly, *kabc* obviously isn't a word. It is in our corpus, though, perhaps because KABC is the ABC television affiliate in Los Angeles.\n",
        "\n",
        "Since the keys are sorted letters, trying to look up a key where the letters are not sorted won't return any words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2arg6JlhVxOe"
      },
      "outputs": [],
      "source": [
        "words[('b', 'c', 'a')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVGX77X9VxOe"
      },
      "source": [
        "This was intentional, though. We need the keys to be standardized so if we want to look up the letter combination `('a', 't', 'c')` we know which order to put them in. This is why we sorted the letters in the tuple before using them as a key when we created the `words` dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzu5Y5A1VxOe"
      },
      "source": [
        "We're almost ready to solve the puzzle. We'll store the center letter in a variable `lc` and the outer letters in a list `lo`. Using the letters from the game board printed above, we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmyDfkARVxOe"
      },
      "outputs": [],
      "source": [
        "lc = 'c'\n",
        "lo = list('btjoka')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JblV5gcGVxOe"
      },
      "source": [
        "We want a list of all possible combinations of these letters that always include the center letter. For example, we'll want to try words containing `[a, b, c]`, `[a, c, t]`, `[a, b, c, t]`, and so on.\n",
        "\n",
        "If we wanted to improve the efficiency of our code, we could also require that each word has at least one vowel. There's no point looking for words containing only the letters `[b, c, t]` because there aren't any. The current problem is small enough, however, that the benefit of doing that is likely to be quite small, so there's no need to incorporate this into our code.\n",
        "\n",
        "The built-in `itertools` module has various functions for getting permutations and combinations from an iterable. We'll use the `combinations(iter, len)` function, which returns all combinations from `iter` of length `len`. For our purposes, we want all combinations of the center letter combined with one or more of the other letters, so we'll choose combinations of `lo` with a length that varies from 1 to 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AorJOw8VxOe"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "combs = [tuple(sorted((lc,)+comb))\n",
        "         for x in range(1,6)\n",
        "         for comb in combinations(lo,x)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdsA1Y28VxOe"
      },
      "source": [
        "`combs` is now a list of possible letter combinations that can be constructed from our 7 letters. Here are the first and last several elements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZDhk6UtVxOe"
      },
      "outputs": [],
      "source": [
        "combs[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-i84vhfVxOe"
      },
      "outputs": [],
      "source": [
        "combs[-3:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6f7-5RJVxOe"
      },
      "source": [
        "We're now ready to look in our word list for each of the candidate letter combinations. We'll iterate over the possible combinations and get the list of words that match the given letter pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8FJCfnJVxOe"
      },
      "outputs": [],
      "source": [
        "rslt = []\n",
        "for comb in combs:\n",
        "    for word in words[comb]:\n",
        "        rslt.append(word)\n",
        "print(sorted(rslt, key=len, reverse=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG0PxUKPVxOf"
      },
      "source": [
        "This works well in finding solutions like *cookbook* or *cockatoo*, but also includes things that aren't words, like *otcbb* and *acto*. Unfortunately there's no good way to fix this without improving our corpus. But we have pretty easily come up with a good list of candidate solutions.\n",
        "\n",
        "Let's put everything together into a function that can solve an arbitrary puzzle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChI5K46tVxOf"
      },
      "outputs": [],
      "source": [
        "def solve_spelling_bee(lc, lo, words, minlen=4):\n",
        "    [lc, lo] = map(str.lower, [lc, lo])\n",
        "    lo = list(lo)\n",
        "    combs = [tuple(sorted((lc,)+comb))\n",
        "             for x in range(1,6)\n",
        "             for comb in combinations(lo,x)]\n",
        "    rslt = []\n",
        "    for comb in combs:\n",
        "        for word in words[comb]:\n",
        "            if len(word)>minlen: rslt.append(word)\n",
        "    return sorted(rslt, key=len, reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP0AXO99VxOf"
      },
      "outputs": [],
      "source": [
        "print(solve_spelling_bee('L', 'ACITVY', words, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBulL_5pVxOf"
      },
      "outputs": [],
      "source": [
        "print(solve_spelling_bee('W', 'GLEHTI', words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB7Fl38SVxOf"
      },
      "source": [
        "As a final step, we can check each of these candidate solutions in an online dictionary. Querying a web server over the internet is much, much slower than processing data within our computer, so this approach has to be done carefully and when there aren't alternative methods available.\n",
        "\n",
        "Here I'm using *The Free Dictionary* to look up words. Before writing this code I use my web browser to try a few words in the dictionary and see how the web site presents results when a word is--or is not--found."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyZsavfMVxOf"
      },
      "outputs": [],
      "source": [
        "def is_word(word):\n",
        "    import requests\n",
        "    url = 'https://www.thefreedictionary.com/'\n",
        "    resp = requests.get(url + word)\n",
        "    if resp.ok:\n",
        "        if resp.url == url + word:\n",
        "            return False if ('Word not found in the Dictionary' in\n",
        "                            resp.text) else True\n",
        "        else:\n",
        "            return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6tSbL_hVxOf"
      },
      "source": [
        "A quick test of the function confirms that it works as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHkd9A12VxOf"
      },
      "outputs": [],
      "source": [
        "is_word('bucolic')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggYmHZYTVxOf"
      },
      "outputs": [],
      "source": [
        "is_word('otcbb')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FuPfgTTVxOf"
      },
      "source": [
        "Note that after checking that the response is valid (that is, that `resp.ok` is `True`), I check that the URL of the response is the same as in the request. I included this because I noticed that the web site sometimes presents results from an encyclopedia when it doesn't find the word in the dictionary, and I want to exclude those.\n",
        "\n",
        "We can now iterate over the list of solutions from the corpus, check each in the online dictionary, and keep only those that appear there. In this example, we reduce the number of solutions from 34 to 19. Since we're pausing for one second between each iteration, this code will take a little longer to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BG54-ZhlVxOf"
      },
      "outputs": [],
      "source": [
        "from time import sleep\n",
        "solutions = []\n",
        "for cand in solve_spelling_bee('W', 'GLEHTI', words):\n",
        "    if is_word(cand):\n",
        "        solutions.append(cand)\n",
        "        sleep(1)\n",
        "\n",
        "print(solutions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syjq4v00VxOf"
      },
      "source": [
        "You may be tempted to write more succinct code here by using a list comprehension, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5qKXnTnVxOf"
      },
      "outputs": [],
      "source": [
        "[cand for cand in solve_spelling_bee('W', 'GLEHTI', words)\n",
        " if is_word(cand)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOG3VLm2VxOf"
      },
      "source": [
        "Doing so, however, would prevent us from calling the sleep function to slow down the repeated web server requests. It is actually possible to build the throttling into the solve_spelling_bee function, but doing so requires some additional features that we will introduce later.\n",
        "\n",
        "Finally, we'll revisit our function, adding the online dictionary lookup along with some additional bells and whistles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5LiMxOaVxOf"
      },
      "outputs": [],
      "source": [
        "def solve_spelling_bee(lc, lo, words, minlen=4, wait=1):\n",
        "    \"\"\"Solves a Spelling Bee game\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    lc : str\n",
        "        single letter at center of board\n",
        "    lo : str\n",
        "        5-character string of other letters\n",
        "    words : dict\n",
        "        dictionary with letter combinations as keys and\n",
        "        list of words as values\n",
        "    minlen : int\n",
        "        minimum length of returned words\n",
        "    wait : float\n",
        "        number of seconds to pause between queries to\n",
        "        online dictionary\n",
        "    \"\"\"\n",
        "    from itertools import combinations\n",
        "\n",
        "    # Require inputs to be correct types\n",
        "    if not (isinstance(lc, str) and len(lc)==1):\n",
        "        print('Must provide a single center letter <lc>')\n",
        "        return None\n",
        "    if not (isinstance(lo, str) and len(lo)==6):\n",
        "        print('Must provide a list of 6 letters <lo>')\n",
        "        return None\n",
        "    if not isinstance(words, dict):\n",
        "        print('Must provide a dictionary of words')\n",
        "        return None\n",
        "\n",
        "    [lc, lo] = map(str.lower, [lc, lo])\n",
        "    lo = list(lo)\n",
        "    combs = [tuple(sorted((lc,)+comb))\n",
        "             for x in range(1,6)\n",
        "             for comb in combinations(lo,x)]\n",
        "    cands = []\n",
        "    for comb in combs:\n",
        "        for word in words[comb]:\n",
        "            if (len(word)>minlen): cands.append(word)\n",
        "    print(f'Found {len(cands)} candidate solutions.')\n",
        "\n",
        "    print('Checking online dictionary...', end=' ')\n",
        "\n",
        "    rslt = []\n",
        "    for cand in cands:\n",
        "        if is_word(cand):\n",
        "            rslt.append(cand)\n",
        "        sleep(wait)\n",
        "    print(f'Found {len(rslt)} solutions.')\n",
        "    return sorted(rslt, key=len, reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_9QGo6qVxOf"
      },
      "outputs": [],
      "source": [
        "solve_spelling_bee('W', 'GLEHTI', words, wait=0.25)"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "108.991px",
        "width": "283.991px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "137px",
        "left": "808.977px",
        "top": "110.284px",
        "width": "195.297px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}